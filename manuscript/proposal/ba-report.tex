\subsection{Current Implementation Overview}\label{subsec:current-implementation}

\begin{table}[!ht]
    \caption{Experimental Results\\All experiments were conducted with a constant privacy budget $\delta = 10^{-5}$, momentum $\beta = 0.9$, weight decay    $\lambda = 10^{-4}$ and a maximum gradient norm of $C = 1.0$.}
    \centering  % Center the table
    \resizebox{\textwidth}{!}{  % Resize the table to fit the width of the page
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
            \hline

            \textbf{Experiment ID} & \textbf{Optimizer} & \textbf{Epochs} & \textbf{Accuracy} & \textbf{Training Time (s)} & \textbf{Privacy Cost} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{Noise Multiplier} \\ [0.5ex]
            \hline\hline
            1                      & SGD                & 100             & 87\%              & -                          & -                     & 0.1                    & 128                 & -                         \\
            2                      & SGD                & 200             & 94\%              & -                          & -                     & 0.1                    & 128                 & -                         \\
            \textbf{3}             & \textbf{DP-SGD}    & \textbf{30}     & \textbf{41\%}     & \textbf{481.49}            & \textbf{3}            & \textbf{0.1}           & \textbf{128}        & \textbf{1.1}              \\
            4                      & DP-SGD             & 30              & 40\%              & 598.68                     & 3                     & 0.2                    & 128                 & 1.1                       \\
            5                      & DP-SGD             & 30              & 39\%              & 527.37                     & 3                     & 0.3                    & 128                 & 1.1                       \\
            6                      & DP-SGD             & 30              & 37\%              & 584.55                     & 3                     & 0.4                    & 128                 & 1.1                       \\
            7                      & DP-SGD             & 30              & 38\%              & 597.60                     & 3                     & 0.5                    & 128                 & 1.1                       \\
            8                      & DP-SGD             & 30              & 35\%              & 995.68                     & 3                     & 0.1                    & 64                  & 1.1                       \\
            \textbf{9}             & \textbf{DP-SGD}    & \textbf{30}     & \textbf{44\%}     & \textbf{473.86}            & \textbf{3}            & \textbf{0.1}           & \textbf{256}        & \textbf{1.1}              \\
            10                     & DP-SGD             & 30              & 44\%              & 597.29                     & 2.99                  & 0.1                    & 512                 & 1.1                       \\
            11                     & DP-SGD             & 30              & 42\%              & 677.04                     & 3                     & 0.1                    & 1024                & 1.1                       \\
            12                     & DP-SGD             & 30              & 43\%              & 519.55                     & 8.01                  & 0.1                    & 128                 & 1.1                       \\
            13                     & DP-SGD             & 30              & 44\%              & 627.49                     & 10.01                 & 0.1                    & 128                 & 1.1                       \\
            14                     & DP-SGD             & 30              & 48\%              & 553.12                     & 50.04                 & 0.1                    & 128                 & 0.1                       \\
            \textbf{15}            & \textbf{DP-SGD}    & \textbf{30}     & \textbf{50\%}     & \textbf{375.37}            & \textbf{50.04}        & \textbf{0.1}           & \textbf{256}        & \textbf{0.1}              \\

            \hline
        \end{tabular}
    } % End of \resizebox
    \label{tab:exp_results}  % Label of the table
\end{table}

\subsection{Best Observed Accuracy and Components/Hyperparameters}\label{subsec:best-accuracy}
In the Experimental testing above, we varied hyperparameters to maximize accuracy of classification on the CIFAR10 dataset. We employed the
($\epsilon$, $\delta$)-Differentially Private - Stochastic Gradient Descent (DP-SGD) as our optimizer. For our loss function, we used the built-in PyTorch CrossEntropy function.
The bolded lines, in the table above, indicate the maximum accuracies achieved by varying each of the hyperparameters. As a baseline, we also implemented
a non-private SGD to give an upper bound on potential accuracy for the CIFAR10 dataset. We were able to achieve 94\% accuracy with SGD over 200 epochs.
\begin{itemize}
    \item \textbf{Learning Rate:} We varied learning rate from 0.1 to 0.5. The smallest learning rate (0.1) yielded the highest accuracy of 41\%.
    \item \textbf{Batch Size:} We varied batch size from 64 to 1024. A batch size of 256 yielded the highest accuracy of 44\% among its peers.
    \item \textbf{Epsilon/Privacy Budget:} Higher epsilons mean lower privacy, but also can mean higher utility. When the epsilon was adjusted to 50, accuracy peaked at 50\%.
    \item \textbf{Noise Multiplier:} Along with Epsilon changes, our highest accuracy (50\%) run also coincided with a decrease in noise multiplier (0.1).
\end{itemize}

\subsection{Failed Approaches}\label{subsec:failed-approaches}
Below is discussion of the approaches that either failed to improve accuracy or reduced the accuracy from our first DP-SGD model run (Experiment ID \#3).

\begin{itemize}
    \item \textbf{Increasing learning rate:} Adjusting the learning rate from 0.3 to 0.5 still yielded lower accuracy, ranging between 37\% and 39\%.
    \item \textbf{Adjusting Batch Sizes:} Decreasing the batch size to 64 was a failure on 2 fronts. It first decreased the accuracy of the model to 35\% and it was noticeably slower than all other approaches at more than 900 seconds to completion. Higher Batch Sizes, near 1000, also experienced some slowdown (more than 650 seconds). Any batch size other than the 256 or 512 seemed to have no positive effect on accuracy. CUDA also indicated a memory warning when batch size was 1024.
\end{itemize}

\subsection{Implementation Challenges}\label{subsec:implementation-challenges}
In the course of completing these tests, we ran into numerous issues. These issues mainly during the implementation of the Opacus Privacy Engine. Our original implementation builds a Residual Network
with 20 layers (Resnet20) and uses non-private SGD as its optimizer.
\begin{itemize}
    \item \textbf{BatchNorm to GroupNorm:} Opacus was not compatible with BatchNorm (which was used in our base implementation \cite{Idelbayev_ResNet20}). To fix this, we employed Opacus' ModuleValidator.fix(model) built-in function, which replaces all the BatchNorms with GroupNorms.
    \item \textbf{Lambda Layer:} Next, we replaced the Lambda layer (from the base implementation \cite{Idelbayev_ResNet20}) with a Shortcut layer, as the Lambda layer was using Serializable functions which weren't compatible with ModuleValidator.fix(model). This occurs because Lambda layers use unnamed functions which are not serializable.
    \item \textbf{Dead Module:} Lastly, we encountered an error with a "dead module". Specifically, while calling the loss.backward() function, our model would return an RuntimeError indicating that we were trying to call the hook of a dead module. This arose due to the base implementations \cite{Idelbayev_ResNet20} usage of torch.nn.DataParallel. To fix this, we removed this parallelization from the code.

\end{itemize}

